{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** This code is quite memory intensive due to the Google News dataset that is used.  About 9 GB of RAM is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from gensim import models\n",
    "from scipy import spatial\n",
    "import numpy as np\n",
    "import os.path\n",
    "import urllib\n",
    "import gzip\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search_tags(entity, search):\n",
    "    \"\"\"\n",
    "    This function searches through all the 'tags' (semantic content) of a data set\n",
    "    and returns 'true' if the search expression is found. case insensitive.\n",
    "    \"\"\"\n",
    "    all_tags = '; '.join([str(x) for x in entity['tags'].values()])\n",
    "    return bool(re.search(search, all_tags, flags=re.IGNORECASE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gunzipFile(inFileName, outFileName):\n",
    "    inF = gzip.open(inFileName, 'rb')\n",
    "    outF = open(outFileName, 'wb')\n",
    "    outF.write( inF.read() )\n",
    "    inF.close()\n",
    "    outF.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the idea for this code comes from this blog post:\n",
    "# http://sujitpal.blogspot.nl/2015/09/sentence-similarity-using-word2vec-and.html\n",
    "def sentenceDistance(sent1, sent2, stoplist):\n",
    "    # remove all non-alphanumeric characters\n",
    "    sent1 = re.sub('[^0-9a-zA-Z]+', ' ', sent1)\n",
    "    sent2 = re.sub('[^0-9a-zA-Z]+', ' ', sent2)\n",
    "    # split up the sentences into tokens, convert to lower case, and remove stopwords\n",
    "    tokens1 = [word for word in sent1.lower().split() if word not in stoplist]\n",
    "    tokens2 = [word for word in sent2.lower().split() if word not in stoplist]\n",
    "    \n",
    "    # get unique tokens\n",
    "    tokens1 = list(set(tokens1))    \n",
    "    tokens2 = list(set(tokens2))    \n",
    "    \n",
    "    # Need to get the shortest distances from all words in sent1 to a word in sent2\n",
    "    # If there are matching words, then the distance is 0\n",
    "    # If a synonym was found, then the distance should be small\n",
    "    # The sum of these shortest distances for all words in sent1 is then returned as totalDist\n",
    "    totalDist = 9999\n",
    "    for token1 in tokens1:\n",
    "        if model.vocab.has_key(token1):\n",
    "            minDist = 9999\n",
    "            for token2 in tokens2:\n",
    "                if model.vocab.has_key(token2):\n",
    "                    lv = model[token1]\n",
    "                    rv = model[token2]\n",
    "                    dist = spatial.distance.cosine(lv, rv)\n",
    "                    # instead of cosine distance can also try euclidean distance\n",
    "                    #dist = spatial.distance.euclidean(lv, rv)\n",
    "                    if dist < minDist:\n",
    "                        minDist = dist\n",
    "            if minDist < 9999:\n",
    "                if totalDist == 9999:\n",
    "                    totalDist = minDist\n",
    "                else: \n",
    "                    totalDist = totalDist + minDist \n",
    "    return(totalDist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the stopwords file.  These are common words which we wish to exclude when performing comparisons (a, an, the, etc).  Every word is separated by a new line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopWordsFile = \"en.txt\"\n",
    "with open(stopWordsFile) as f:\n",
    "    stoplist = [x.strip('\\n') for x in f.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to check if we have the word2vec model which has been pre-trained on the Google News corpus.  The vectors are 300 dimentions and this is generated with a training set involving over 100 billion words\n",
    "\n",
    "**Note:** This file is 1.6 GB compressed and expands to 3.4 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if os.path.isfile(\"GoogleNews-vectors-negative300.bin.gz\") == False:\n",
    "    # This is the direct download link for GoogleNews-vectors-negative300.bin.gz\n",
    "    # If the link changes, just search for the filename as this is a file often used for word2vec\n",
    "    downloadURL = 'https://doc-0g-8s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/dhu4deogg9hg0tkm9tdann504ue0vp91/1461232800000/06848720943842814915/*/0B7XkCwpI5KDYNlNUTTlSS21pQmM?e=download'\n",
    "    urllib.urlretrieve (downloadURL, \"GoogleNews-vectors-negative300.bin.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unzip the file.  This may take a few several minutes due to the python gzip library.  It may be quicker to just do this from the command line or do a system call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if os.path.isfile(\"GoogleNews-vectors-negative300.bin\") == False:\n",
    "    gunzipFile('GoogleNews-vectors-negative300.bin.gz', 'GoogleNews-vectors-negative300.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a model using this pre-trained data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = models.Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the data from the catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# http://stackoverflow.com/questions/956867/how-to-get-string-objects-instead-of-unicode-ones-from-json-in-python\n",
    "# need this to deal with unicode errors\n",
    "def byteify(input):\n",
    "    if isinstance(input, dict):\n",
    "        return {byteify(key): byteify(value)\n",
    "                for key, value in input.iteritems()}\n",
    "    elif isinstance(input, list):\n",
    "        return [byteify(element) for element in input]\n",
    "    elif isinstance(input, unicode):\n",
    "        return input.encode('utf-8')\n",
    "    else:\n",
    "        return input\n",
    "\n",
    "gunzipFile('../catalogs/gabi_2016_professional-database-2016.json.gz', \n",
    "           '../catalogs/gabi_2016_professional-database-2016.json')\n",
    "gunzipFile('../catalogs/uslci_ecospold.json.gz', \n",
    "           '../catalogs/uslci_ecospold.json')\n",
    "\n",
    "with open('../catalogs/gabi_2016_professional-database-2016.json') as data_file:    \n",
    "    gabi = json.load(data_file, encoding='utf-8')\n",
    "\n",
    "with open('../catalogs/uslci_ecospold.json') as data_file:    \n",
    "    uslci = json.load(data_file, encoding='utf-8')\n",
    "    \n",
    "gabi = byteify(gabi)\n",
    "uslci = byteify(uslci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process name to match:\n",
      "Roundwood, softwood, average, at forest road, NE-NC\n",
      "Matches using Word2Vec:\n",
      "('Timber cedar (12% moisture; 10.7% H2O content) (m3)', 4.3803496516921951)\n",
      "('Timber spruce (12% moisture; 10.7% H2O content)', 4.3803496516921951)\n",
      "('Timber (12% moisture; 10.7% H2O content)', 4.3803496516921951)\n",
      "('Timber pine (65% moisture; 40% H2O content)', 4.5251883015105836)\n",
      "('Timber spruce (65% moisture; 40% H2O content)', 4.5282608796782853)\n",
      "('Road (average)', 4.5344290329591512)\n",
      "('Laminated veneer lumber (LVL)', 4.6418420007844299)\n",
      "('Wood pellets (5.8% H2O content)', 4.6981787422455534)\n",
      "('Waste incineration of untreated wood (10.7% H2O content)', 4.7205080571528928)\n",
      "('Solid construction timber (15% moisture)', 4.7382955596134773)\n"
     ]
    }
   ],
   "source": [
    "roundwood = [flow for flow in uslci['flows'] if search_tags(flow,'roundwood, softwood')]\n",
    "roundwoodExample = roundwood[0]\n",
    "\n",
    "# number of top scores to show\n",
    "numTopScores = 10\n",
    "\n",
    "flowNames = []\n",
    "distValues = []\n",
    "for flow in gabi['archives'][0]['flows']:\n",
    "    name = flow['tags']['Name']\n",
    "    flowNames.append(name)\n",
    "    dist = sentenceDistance(roundwoodExample['tags']['Name'], name, stoplist)\n",
    "    distValues.append(dist)\n",
    "\n",
    "len(flowNames)\n",
    "    \n",
    "# figure out top scores\n",
    "arr = np.array(distValues)\n",
    "topIndices = arr.argsort()[0:numTopScores]\n",
    "topScores = np.array(distValues)[topIndices]\n",
    "\n",
    "print 'Process name to match:'\n",
    "print roundwoodExample['tags']['Name']\n",
    "\n",
    "print 'Matches using Word2Vec:'\n",
    "for i, s in zip(topIndices, topScores):\n",
    "    if s < 9999:\n",
    "        print(flowNames[i],s)\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
